{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-site Domain Adaptation for rs-fMRI Classifications\n",
    "\n",
    "Recent applications of deep learning techniques have shown promising results in conducting various predictive and classification based analysis of medical imagery, especially using **resting-state fMRI** (rs-fMRI). In common practices, it is seen that supervised DL algorithms perform better when large datasets are utilized for training them.\n",
    "\n",
    "However, large rs-fMRI datasets are difficult to acquire from a single site and hence, one database may contain data collected from multiple sites/sources. While this increases the amount of data that can be used by a DL model, the inherent differences in acquisition method and subjectsâ€™ demographics among the sites introduces a bias termed as **batch effects**, which could impact the training capacity of a DL model.\n",
    "\n",
    "\n",
    "**Multi-site DA models** aim to extract site-invariant features by mapping the neuroimaging data from different sites onto a shared space which may lead to better information retention while reducing the problems faced by batch effects. To show the viability of multi-site DA models, public datasets like `ABIDE` and `ADHD200` were used for classification tasks such as control vs illness  and male vs female. DA Models such as `DANN, MDAN, DARN, MDMN and MSDA` were compared against baseline DL models. \n",
    "\n",
    "\n",
    "In this notebook the code for the experiments carried out in (cite) are provided. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from torch.utils.data.sampler import SubsetRandomSampler,RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader      \n",
    "import scipy.io as sio\n",
    "import math\n",
    "from easydict import EasyDict as ED\n",
    "import copy\n",
    "import os\n",
    "from nilearn.regions import RegionExtractor\n",
    "from nilearn.decomposition import DictLearning\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import nibabel\n",
    "import pickle\n",
    "import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.autograd import Function\n",
    "import imblearn \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "import nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing custom modules\n",
    "\n",
    "These modules provide utilty functions along with classes of each of the DA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import DarnMLP, DarnConv, MdmnMLP, MdmnConv, MsdaMLP, MsdaConv\n",
    "from load_data import load_fmri_data, multi_data_loader, data_loader\n",
    "import utils\n",
    "from utils import set_configs, OverSampler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter and arguments setting\n",
    "\n",
    "The following code contains the hyperparameter settings which are independent of the data related information, namely:\n",
    "\n",
    "1. `classifier_config` : Hyperparams for the classifier model which classifies the labels of the data.\n",
    "\n",
    "2. `domain_config` : Hyperparams for the domain classifier model which is used for certain DA models to introduce domain invariancy\n",
    "\n",
    "3. `args` : global hyperparams that would be used throughout the expermiment to configure various aspects of data streaming, training and testing.\n",
    "\n",
    "Note: args for `ADHD` and `ABIDE` are provided separately for convinience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_config = {\n",
    "    'input_dim' : 1000,\n",
    "    # 'hidden_layers' : [512,256,100],\n",
    "    'hidden_layers' : [100],\n",
    "    'output_dim' : 2,\n",
    "    'drop_rate' : 0.5,\n",
    "    'process_final' : False,\n",
    "    'lr': 1e-4,\n",
    "    'decay': 0,  # L-2 regularization\n",
    "    'batch_size' : 35,\n",
    "    'epochs' : 100\n",
    "    }\n",
    "\n",
    "domain_config = {\n",
    "    'input_dim' : 1000,\n",
    "    'hidden_layers' : [100],\n",
    "    # 'hidden_layers' : [512,256,100],\n",
    "    'output_dim' : 1,\n",
    "    'drop_rate' : 0.5,\n",
    "    'process_final' : False,\n",
    "    'lr': 1e-4,\n",
    "    'decay': 0,\n",
    "    'batch_size' : 35,\n",
    "    'epochs' : 100\n",
    "    }\n",
    "\n",
    "# args = ED({\n",
    "#   \"name\" : \"ABIDE\",\n",
    "#   \"method\" : \"mdmn\",\n",
    "#   \"data_path\" : \"data/ABIDE_aal_allData.pkl\",\n",
    "#   \"result_path\": \"Torch_cache/model_ckpts/DOM_ADAP_CV\",\n",
    "#   \"mode\" : \"L2\",\n",
    "#   \"lr\" : 1e-4,\n",
    "#   \"mu\" : 1,\n",
    "#   \"gamma\" : 0.5,\n",
    "#   \"epoch\" : 100,\n",
    "#   \"batch_size\" : 100,\n",
    "#   \"cuda\" : 0,\n",
    "#   \"seed\" : 0,\n",
    "#   \"folds\" : 10,\n",
    "#   \"mod\" : 1,\n",
    "#   \"sampling\" : None\n",
    "# })\n",
    "\n",
    "args = ED({\n",
    "  \"name\" : \"ADHD\",\n",
    "  \"method\" : \"mdmn\",  # Options: \"dann\", \"darn\", \"mdmn\", \"msda\"\n",
    "  \"data_path\" : \"data/ADHD_aal_allData.pkl\",\n",
    "  \"result_path\": \"Torch_cache/model_ckpts/DOM_ADAP_CV\",\n",
    "  \"mode\" : \"L2\", # Options: \"dynamic\" and \"L2\" for DARN only.\n",
    "  \"lr\" : 3e-4,\n",
    "  \"mu\" : 3,\n",
    "  \"gamma\" : 0.6,\n",
    "  \"epoch\" : 50,\n",
    "  \"batch_size\" : 200,\n",
    "  \"cuda\" : 0,\n",
    "  \"seed\" : 0,\n",
    "  \"folds\" : 10, # for stratified K-Fold splitting\n",
    "  \"mod\" : 0, # flip to 1 for oversampling\n",
    "  \"sampling\" : None # None : 50/50 class distribution for all domains; [0.0,0.9] : ratio of (major_class/total_data) wherever possible \n",
    "  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Train and Test loop\n",
    "\n",
    "Next, the args and params dependent on the data used for training and testing are set.\n",
    "After setting up the enviornment, the iterations take place for each site in a 10-fold stratified cross validation style.\n",
    "\n",
    "Note : the params related to feature extractor are also set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "\n",
    "######################## logger, Seed and Folds ################################\n",
    "\n",
    "device = torch.device(\"cuda:%d\" % args.cuda if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = args.batch_size\n",
    "\n",
    "result_path = os.path.join(args.result_path,\n",
    "                          args.name,\n",
    "                          args.method,\n",
    "                          args.mode,\n",
    "                          'lr_'+str(args.lr),\n",
    "                            )\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "logger = utils.get_logger(os.path.join(result_path,\n",
    "                                      \"gamma_%g_seed_%d.log\" % (args.gamma,\n",
    "                                                                args.seed)))\n",
    "logger.info(\"Hyperparameter setting = %s\" % args)\n",
    "\n",
    "# Set random number seed.\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "skf = StratifiedKFold(args.folds, random_state = args.seed, shuffle=True)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "#################### Loading the datasets ######################################\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "# the insts and labels are now in dict format\n",
    "data_names, train_insts, train_labels, test_insts, test_labels = load_fmri_data(args.name,\n",
    "                                                                                          args.data_path,\n",
    "                                                                                          logger,mod = args.mod,\n",
    "                                                                                          alpha = args.sampling)\n",
    "\n",
    "if args.method == \"src+\":\n",
    "  train_insts, train_labels, test_insts, test_labels = add_site_info(train_insts, train_labels, test_insts, test_labels)\n",
    "\n",
    "configs = set_configs(train_insts[list(train_insts.keys())[0]].shape[-1], len(data_names) - 1)\n",
    "\n",
    "configs[\"mode\"] = args.mode\n",
    "configs[\"mu\"] = args.mu\n",
    "configs[\"gamma\"] = args.gamma\n",
    "configs[\"num_src_domains\"] = len(data_names) - 1\n",
    "num_datasets = len(data_names)\n",
    "configs['moments'] = 5\n",
    "logger.info(\"Time used to process the %s = %g seconds.\" % (args.name, time.time() - time_start))\n",
    "logger.info(\"-\" * 100)\n",
    "\n",
    "test_results = {}\n",
    "np_test_results = np.zeros(num_datasets)\n",
    "target_c_dict = {}\n",
    "################################################################################\n",
    "\n",
    "\n",
    "##################### misc settings ############################################\n",
    "\n",
    "if args.method in [\"dann\", \"src\", \"tar\",\"src+\"]:\n",
    "    # Combine all sources for these methods\n",
    "    num_src_domains = configs[\"num_src_domains\"] = 1\n",
    "else:\n",
    "  num_src_domains = configs[\"num_src_domains\"]\n",
    "\n",
    "logger.info(\"Model setting = %s.\" % configs)\n",
    "if args.method in ['src', 'tar',\"src+\"]:\n",
    "  args.mu = configs[\"mu\"] =  0\n",
    "if args.method == \"dann\":\n",
    "  args.mode = configs[\"mode\"] = \"dynamic\"\n",
    "\n",
    "################################################################################\n",
    "\n",
    "###################### Train/Test Loops ########################################\n",
    "\n",
    "alpha_list = np.zeros([num_datasets, num_src_domains, args.folds,args.epoch])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_datasets):\n",
    "\n",
    "  logger.info('########### Site: %s #############' % data_names[i])\n",
    "\n",
    "  #------- src tgt data split-----------------------------------#\n",
    "  \n",
    "  source_insts = copy.deepcopy(train_insts)\n",
    "  source_labels = copy.deepcopy(train_labels)\n",
    "  _,target_insts = source_insts.pop(i,None), test_insts.pop(i,None)\n",
    "  _,target_labels = source_labels.pop(i,None), test_labels.pop(i,None)\n",
    "\n",
    "  if args.method == \"dann\" or args.method == \"src\":\n",
    "    tmp_insts = []\n",
    "    tmp_labels = []\n",
    "    for key in source_insts.keys():\n",
    "      tmp_insts.extend(source_insts[key])\n",
    "      tmp_labels.extend(source_labels[key])\n",
    "    new_inst,new_labels = {},{}\n",
    "    new_inst[0] = np.squeeze(np.array(tmp_insts))\n",
    "    new_labels[0] = np.array(tmp_labels)\n",
    "    source_insts = new_inst\n",
    "    source_labels = new_labels\n",
    "  if args.method == \"tar\":\n",
    "    source_insts, source_labels = {},{}\n",
    "    source_insts[0] = target_insts\n",
    "    source_labels[0] = target_labels\n",
    "\n",
    "  #---------------------------------------------------------------#\n",
    "  \n",
    "  target_c = np.zeros((2,2)) # confusion matrix for each domain\n",
    "  target_acc = 0.0\n",
    "  target_feats = []\n",
    "  # print(np.unique(target_labels,return_counts=True))\n",
    "  #-------------------10foldCV-------------------------------------#\n",
    "  for fold,(train_idx, test_idx) in enumerate(skf.split(target_insts,target_labels)):\n",
    "    # print(np.shape(target_insts),np.shape(train_idx),np.shape(test_idx))\n",
    "\n",
    "    #----------- Model selection -----------------------------------#\n",
    "\n",
    "    if args.method == 'mdmn':\n",
    "      # print('using mdmn')\n",
    "      model = MdmnMLP(configs, classifier_config, domain_config).to(device)\n",
    "    elif args.method == 'msda':\n",
    "      model = MsdaMLP(configs, classifier_config).to(device)\n",
    "    else:\n",
    "      # print('using darn')\n",
    "      model = DarnMLP(configs, classifier_config, domain_config).to(device)\n",
    "    \n",
    "    #---------------------------------------------------------------#\n",
    "\n",
    "    #------------------- Optimizer ---------------------------------#\n",
    "    if args.method != 'msda':\n",
    "      optimizer = optim.Adagrad(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "      opt_G = optim.Adagrad(model.G_params, lr=args.lr)\n",
    "      opt_C1 = optim.Adagrad(model.C1_params, lr=args.lr)\n",
    "      opt_C2 = optim.Adagrad(model.C2_params, lr=args.lr)\n",
    "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer,max_lr = 1e-3, epochs=args.epoch, steps_per_epoch=len(train_idx))\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode = 'max',patience=5)\n",
    "    #---------------------------------------------------------------#\n",
    "  \n",
    "\n",
    "    \n",
    "    logger.info(\"-\"*10)\n",
    "    logger.info(\"Starting Fold %d\" %(fold+1) )\n",
    "    logger.info(\"-\"*10)\n",
    "\n",
    "    time_start = time.time()\n",
    "    max_acc = 0.0\n",
    "    for t in range(args.epoch):\n",
    "      #--------------------Training---------------------------------#\n",
    "      if args.method ==\"tar\":\n",
    "        train_loader =  data_loader(target_insts[train_idx], \n",
    "                                target_labels[train_idx],\n",
    "                                batch_size=batch_size, shuffle=True)\n",
    "      else:\n",
    "        train_loader = multi_data_loader(source_insts, source_labels, batch_size)\n",
    "        \n",
    "      running_loss = 0.0\n",
    "      feats = []\n",
    "      c = np.zeros((2,2))        \n",
    "      model.train()\n",
    "\n",
    "      for xs, ys in train_loader:\n",
    "\n",
    "        if args.method != \"tar\":\n",
    "          for j in range(num_src_domains):\n",
    "  \n",
    "            xs[j] = torch.squeeze(torch.tensor(xs[j], \n",
    "                                              requires_grad=False)).to(device).float()\n",
    "            ys[j] = torch.tensor(ys[j], requires_grad=False).to(device).long()\n",
    "        \n",
    "        else:\n",
    "          xs = [torch.reshape(torch.tensor(xs, requires_grad=False, \n",
    "                            dtype=torch.float32).to(device).float(),(xs.shape[0],-1))]\n",
    "          ys = [torch.tensor(ys, requires_grad=False,\n",
    "                            dtype=torch.long).to(device)]    \n",
    "            \n",
    "        ridx = np.random.choice(train_idx, batch_size)\n",
    "        tinputs = target_insts[ridx, :]\n",
    "        tinputs = torch.tensor(tinputs, \n",
    "                              requires_grad=False).to(device).float().squeeze()\n",
    "        \n",
    "        # print(np.shape(xs),np.shape(ys),np.shape(tinputs) ) #debugging\n",
    "        if args.method != 'msda':\n",
    "          optimizer.zero_grad()\n",
    "          loss, alpha = model(xs, ys, tinputs)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # scheduler.step()\n",
    "        else:\n",
    "          loss = utils.msda_train_step(model, xs, ys,\n",
    "                                      tinputs, opt_G, opt_C1, opt_C2)\n",
    "        running_loss += loss.item()\n",
    "      #--------------------------------------------------------------#\n",
    "\n",
    "      #--------------------Testing-----------------------------------#\n",
    "\n",
    "      model.eval()\n",
    "      test_loader = data_loader(target_insts[test_idx], \n",
    "                                target_labels[test_idx],\n",
    "                                batch_size=len(test_idx), shuffle=False)\n",
    "      test_acc = 0.\n",
    "      \n",
    "      for xt, yt in test_loader:\n",
    "          xt = torch.tensor(xt, requires_grad=False, \n",
    "                            dtype=torch.float32).to(device).squeeze()\n",
    "          yt = torch.tensor(yt, requires_grad=False,\n",
    "                            dtype=torch.int64).to(device)\n",
    "          # print(np.shape(xt),np.shape(yt))\n",
    "          preds_labels = torch.max(model.inference(xt), 1)[1]\n",
    "          # _,preds_labels = torch.max(preds_labels, dim =1)\n",
    "          # print(np.shape(preds_labels),np.shape(yt))\n",
    "          feats.extend(model.get_feats(xt).detach().cpu().numpy())\n",
    "          for ii in range(len(preds_labels)):\n",
    "            c[preds_labels[ii]][yt[ii]] += 1\n",
    "          \n",
    "          test_acc += torch.sum(preds_labels == yt).item()\n",
    "      test_acc /= target_insts[test_idx].shape[0]\n",
    "      # scheduler.step(test_acc)\n",
    "\n",
    "      if max_acc<test_acc:\n",
    "        max_acc = test_acc\n",
    "        max_c = c\n",
    "        best_feats = feats\n",
    "        # feat_path = os.path.join(result_path,\"%s_gamma_%g_seed_%d_%s_%s_feat.npy\" \n",
    "        #                          % (data_names[i],args.gamma,\n",
    "        #                           args.seed,\n",
    "        #                           args.method,\n",
    "        #                           args.mode))\n",
    "        # np.save(feat_path, best_feats)\n",
    "        logger.info(\"Epoch[%d/%d] | Train_loss : %.7f | Acc: %.3f\" %(t, args.epoch,\n",
    "                                                                    running_loss,\n",
    "                                                                    max_acc))\n",
    "        if args.method == 'mdmn' or (args.method == 'darn' and args.mode == 'L2'):\n",
    "          logger.info(\"Epoch %d, Fold %d, Alpha on %s: %s\" % (t,fold ,data_names[i], alpha))\n",
    "          alpha_list[i, :, fold, t] = alpha\n",
    "\n",
    "      if max_acc == 1.0: #stop if acc 100%\n",
    "        break\n",
    "      #------------------------------------------------------------#\n",
    "    target_acc += max_acc\n",
    "    target_c += max_c\n",
    "    target_feats.extend(best_feats)\n",
    "  target_acc /= args.folds\n",
    "  \n",
    "\n",
    "  feat_path = os.path.join(result_path,\n",
    "                          \"%s_gamma_%g_seed_%g_mu_%g_feats.pkl\" % (data_names[i], args.gamma,\n",
    "                                                                    args.seed, args.mu))\n",
    "  conf_path = os.path.join(result_path,\n",
    "                          \"gamma_%g_seed_%g_mu_%g_conf.pkl\" % ( args.gamma,\n",
    "                                                                    args.seed, args.mu))\n",
    "  target_c_dict[data_names[i]] = target_c \n",
    "  print(target_c)\n",
    "\n",
    "  with open(feat_path,'wb') as fp:\n",
    "    pickle.dump(target_feats,fp)\n",
    "  with open(conf_path,'wb') as fp:\n",
    "    pickle.dump(target_c_dict,fp)\n",
    "    \n",
    "  test_results[data_names[i]] = target_acc\n",
    "  np_test_results[i] = target_acc\n",
    "  # matrices[data_names[i]] = target_c\n",
    "  # logger.info(\"%s Confusion Matrix: %s\" % (data_names[i],str(target_c)))\n",
    "  logger.info(\"%s Accuracy: %.4f\" %(data_names[i], target_acc))\n",
    "\n",
    "logger.info(\"Test Results: %s\" % str(test_results))\n",
    "# logger.info(\"Conf Matrices: %s\" % str(matrices))\n",
    "################################################################################  \n",
    "\n",
    "\n",
    "################# Save results to files ########################################\n",
    "test_file = os.path.join(result_path,\n",
    "                        \"gamma_%g_seed_%d_test.txt\" % (args.gamma,\n",
    "                                                        args.seed))\n",
    "np.savetxt(test_file, np_test_results, fmt='%.6g')\n",
    "# test_file2 = os.path.join(result_path,\n",
    "#                         \"gamma_%g_seed_%d_%s_%s_matrices.pkl\" % (args.gamma,\n",
    "#                                                         args.seed,\n",
    "#                                                         args.method,\n",
    "#                                                         args.mode))\n",
    "\n",
    "\n",
    "\n",
    "# with open(test_file2,'wb') as fp:\n",
    "#   pickle.dump(matrices,fp)\n",
    "# fp.close()\n",
    "\n",
    "# if args.method == 'mdmn' or (args.method == 'darn' and args.mode == 'L2'):\n",
    "#     for i in range(num_datasets):\n",
    "#         alpha_file = os.path.join(result_path,\n",
    "#                                   \"gamma_%g_seed_%d_alpha%d.txt\" % (args.gamma,\n",
    "#                                                                     args.seed,\n",
    "#                                                                     i))\n",
    "#         np.savetxt(alpha_file, alpha_list[i], fmt='%.6g')\n",
    "\n",
    "################################################################################\n",
    "accs = test_results\n",
    "mean_acc = []\n",
    "for k,v in accs.items():\n",
    "  mean_acc.append(v)\n",
    "logger.info('mean Acc:%.6f' % np.mean(mean_acc))\n",
    "logger.info(\"Done\")\n",
    "logger.info(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
